version: '3.9'

services:
  client:
    build:
      context: ./client
      dockerfile: Dockerfile
    ports:
      - "3000:3000"
    environment:
      - API_URL=http://backend:8000
    depends_on:
      - backend
    networks:
      - app_network

  backend:
    build:
      context: ./server
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    env_file:
      - .env # Указываем путь к файлу .env
    networks:
      - app_network
    depends_on:
      - weaviate
    environment:
      - WEAVIATE_URL=http://weaviate:8080
    restart: "on-failure:10"

  weaviate:
    image: cr.weaviate.io/semitechnologies/weaviate:1.27.7
    container_name: weaviate
    ports:
      - "8080:8080"
      - "50051:50051"
    environment:
      QUERY_DEFAULTS_LIMIT: "20"
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: "true"
      PERSISTENCE_DATA_PATH: "./data"
      ENABLE_MODULES: "text2vec-transformers"
      DEFAULT_VECTORIZER_MODULE: "text2vec-transformers"
      TRANSFORMERS_INFERENCE_API: "http://transformers-inference:8080"
    networks:
      - app_network
    depends_on:
      - transformers-inference

  transformers-inference:
    image: cr.weaviate.io/semitechnologies/transformers-inference:sentence-transformers-multi-qa-MiniLM-L6-cos-v1
    container_name: transformers-inference
    ports:
      - "8081:8080"
    environment:
      ENABLE_CUDA: "0" # Set to "1" if you have a GPU
      TRANSFORMERS_MODEL: "sentence-transformers/all-MiniLM-L6-v2" # Use a Hugging Face model
    networks:
      - app_network

volumes:
  db_user:

networks:
  app_network:
    driver: bridge